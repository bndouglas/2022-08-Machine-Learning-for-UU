{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23dcca9b-3523-453e-9cc7-26bfcaa0f018",
   "metadata": {},
   "source": [
    "<h1 style='color: #C9C9C9'>Machine Learning with Python<img style=\"float: right; margin-top: 0;\" width=\"240\" src=\"../../Images/cf-logo.png\" /></h1> \n",
    "<p style='color: #C9C9C9'>&copy; Coding Fury 2022 - all rights reserved</p>\n",
    "\n",
    "<hr style='color: #C9C9C9' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e6660",
   "metadata": {},
   "source": [
    "# What is Logistic Regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d2aeab",
   "metadata": {},
   "source": [
    "Logistic Regression is used when the target variable can be one of 2 outcomes: True or False\n",
    "\n",
    "For example, Logistic Regression could be used to determine if an email was spam or not spam. Or it could be used to determine if someone has a particular disease based on a range of factors. \n",
    "\n",
    "\n",
    "In this tutorial we're going to predict if a patient has diabetes or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822edf2",
   "metadata": {},
   "source": [
    "# The Diabetes Dataset\n",
    "\n",
    "The Diabetes Dataset has 8 features\n",
    "1. Pregnancies: the number of times the patient has been pregnant\n",
    "1. Glucose : blood glucose levels from a glucose tolerance test\n",
    "1. BloodPressure : diastolic blood pressure values (mm Hg)\n",
    "1. SkinThickness : triceps skin fold thickness (mm)\n",
    "1. Insulin : levels of insulin\n",
    "1. BMI : body mass index\n",
    "1. Age : age in years\n",
    "\n",
    "\n",
    "The Target Value is \n",
    "* Outcome : whether the patient has diabetes or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b1bab-5269-4990-9a03-c31f8e0a9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diabetes_df = pd.read_csv('../../Data/diabetes.csv')\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df['BMI'].values.reshape(-1,1)\n",
    "y = diabetes_df['Outcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553569b",
   "metadata": {},
   "source": [
    "# Simple Example: BMI and Outcome\n",
    "\n",
    "To make this example as simple as possible, we're going to perform logistic regression on a single feature, the BMI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41436423",
   "metadata": {},
   "source": [
    "# Introduction to Logistic Regression\n",
    "\n",
    "Regression is about finding the line of best fit. You might wonder how this works with only 2 possible outcomes? \n",
    "\n",
    "The concept is that the Logistic Algorithm tries to find the best fit for the dat based on an s-curve as shown below. \n",
    "\n",
    "![S-Curve](../../Images/logistic-regression-s-curve.png)\n",
    "\n",
    "## The Logistic Regression Equation\n",
    "\n",
    "The general Equation for a Sigmoid Curve (or s-curve) is \n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "And you may recall that the equation for linear regression is \n",
    "$$a_1x_1 + a_2x_2 + ... + a_nx_n + b$$\n",
    "\n",
    "**Putting these together, we get the equation used for logistic regression:**\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-(a_1x_1 + a_2x_2 + ... + a_nx_n + b)}}$$\n",
    "\n",
    "In our example, the s-curve gives the probability that a patient has diabetes. Values along the s-curve therefore range from 0 to 1.\n",
    "\n",
    "## Logistic Regression as a Classifier\n",
    "\n",
    "Generally speaking, we tend to use Logistic Regression as a binary classifier. That is to say, our predictions should fall into one of two classes; having diabetes, or not.\n",
    "\n",
    "![Logistic Regression Classifier](../../Images/logistic-regression-as-a-classifier.png)\n",
    "\n",
    "By default, a 'threshold' is set at 0.5. If the probability of a patient having diabetes is greater than 0.5, the algorithm will classify that patient as having diabetes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1cba0",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f289df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df['BMI'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes_df['Outcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada782d",
   "metadata": {},
   "source": [
    "## Visualise the training data\n",
    "\n",
    "By plotting the values for BMI and outcome, we can see the data that we're trying to fit a s-curve to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ca8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X,y, marker='>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349aa823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = LogisticRegression() \n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022076f6",
   "metadata": {},
   "source": [
    "# Visualise the Model\n",
    "\n",
    "Let's see how the model predicts for each of our input observations by visualising the predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa949e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X,y, marker='>', s=100)\n",
    "plt.scatter(X, y_pred, color='red', marker='o', s=10, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b05c41",
   "metadata": {},
   "source": [
    "The chart shows that our algorithm has determined that if a patient's BMI is greater than 39, then for sure, they've got diabetes. \n",
    "\n",
    "I didn't realise you could have a BMI this large!\n",
    "\n",
    "You can also see from the chart that there are plenty of observations with BMI larger than this where the patient doesn't have Diabetes, so BMI alone probably isn't the full story. \n",
    "\n",
    "For now, we'll stick to just analysing BMI, but this time we'll train our model using a train, test split, and try to determine the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cd977",
   "metadata": {},
   "source": [
    "# Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21)\n",
    "\n",
    "model = LogisticRegression() \n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test) # The accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09385c4",
   "metadata": {},
   "source": [
    "As you can see, our model accuracy is 62%. \n",
    "\n",
    "But how is this calculated? Different models score accuracy in different ways. In the case of Linear Regression, we used the Residual Sum of Squares (RSS) to determine the accuracy. \n",
    "\n",
    "To understand accuracy in terms of a Logistic Regression classifier, we need to start by understanding a Confusion Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef34cc-6167-476a-bd9a-b9fab2afc4b0",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc9e8b",
   "metadata": {},
   "source": [
    "Let's make our confusion matrix look a bit nicer, with a Seaborn heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plotting the confusion matrix\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.xlabel(\"Predicted Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49fa26",
   "metadata": {},
   "source": [
    "### A Confusion Matrix works like this: \n",
    "\n",
    "In the top left we have coordinates actual=0, predicted=0.\n",
    "* The actual value was 0, and we predicted 0. \n",
    "* This happened 131 times\n",
    "* This is known as a **True Negative**. (We predicted Negative correctly)\n",
    "\n",
    "In the top right we have coordinates actual=0, predicted=1\n",
    "* The actual value was 0, but we predicted 1\n",
    "* This happened 13 times\n",
    "* This is known as a **False Negative**. (We predicted Negative incorrectly).\n",
    "* A False Negative is also known as **Type 2 Error**\n",
    "\n",
    "In the bottom left we have coordinates actual=1, predicted=0\n",
    "* The actual value was 1, but we predicted 0\n",
    "* This happened 48 times\n",
    "* This is known as a **False Positive**. (We predicted Positive incorrectly).\n",
    "* A False Positive is also known as a **Type 1 Error**\n",
    "\n",
    "In the bottom right we have coordinates actual=1, predicted=1\n",
    "* The actual value was 1, and we predicted 1\n",
    "* This happened 39 times\n",
    "* This is known as a **True Positive**. (We predicted Positive correctly).\n",
    "\n",
    "![Confusion Matrix](../../Images/confusion-matrix.jpeg)\n",
    "\n",
    "Although I prefer this example from [Answer Miner](https://www.answerminer.com/blog/confusion-matrix-explained)\n",
    "\n",
    "![Confusion Matrix - Preganancy Example](../../images/confusion_matrix_pregnant.jpeg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46aa2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9624b0a",
   "metadata": {},
   "source": [
    "# Evaluating Performance\n",
    "\n",
    "The following metrics are widely used when predicting model performance.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "The accuracy is defined as the ratio between the number of correct predictions and the total number of predictions.\n",
    "\n",
    "$$ accuracy = \\frac{TP + TN}{TP + FP + TN + FN}$$\n",
    "\n",
    "This can also be represented as:\n",
    "\n",
    "$$ accuracy = \\frac{correct \\space predictions}{total \\space observations} $$\n",
    "\n",
    "### Precision - the percentage of positive predictions that were correct\n",
    "\n",
    "For each class it is defined as the ratio of true positives to the sum of true positives and false negatives i.e. the ratio between observations correctly predicted to be positive and the sum of the observations that were correctly identified as positive plus those incorrectly identified as positive. \n",
    "\n",
    "$$ precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "\n",
    "### Recall\n",
    "\n",
    "**The percentage of positive cases we managed to catch**\n",
    "\n",
    "Recall is the ability of a classifier to find all positive instances. Also known as Sensitivity or the True Positive rate.\n",
    "\n",
    "This is the ratio between observations correctly predicted to be positive and the sum of the observations that were correctly identified as positive plus those incorrectly identified as negative. \n",
    "\n",
    "$$ recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "### Specificity - the percentage of negative chases we managed to catch\n",
    "\n",
    "Also known as True Negative Rate.\n",
    "\n",
    "This is the ratio between observations correctly predicted to be negative and the sum of the observations that were correctly identified as negative plus those incorrectly identified as positive. \n",
    "\n",
    "$$ specificity = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "\n",
    "\n",
    "### F1 Score - the percentage of positive predictions that were correct\n",
    "\n",
    "$$ f1 score = 2 \\times\\frac{precision \\times recall }{precision + recall}$$\n",
    "\n",
    "\n",
    "For more information see this Wikipedia page on [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "Aslo check out this *Towards Data Science* Tutorial on [Understanding the Confusion Matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d73d3",
   "metadata": {},
   "source": [
    "# Performance Metrics for our Diabetes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa918012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "print(f\"Accuracy for test set is {metrics.accuracy_score(y_test, y_pred):0.4f}.\")\n",
    "print(f\"Precision for test set is {metrics.precision_score(y_test, y_pred):0.4f}.\")\n",
    "print(f\"Recall for test set is {metrics.recall_score(y_test, y_pred):0.4f}.\")\n",
    "print(f\"F1 Score for test set is {metrics.f1_score(y_test, y_pred):0.4f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa9df97",
   "metadata": {},
   "source": [
    "Alternatively you can get use the inbuilt classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791bf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8c880cd",
   "metadata": {},
   "source": [
    "# ROC Curves\n",
    "\n",
    "Another evaluation technique we can use for this then is the Receiver Operating Characteristic (ROC) curve which is seen in a lot of Machine Learning Contexts. \n",
    "\n",
    "This is used to plot the true positive rate against the false positive rate, showing the tradeoff between sensitivity and specificity.\n",
    "\n",
    "Within this plot the ideal is the purple line with the result from a purely random model is the red line, the aim being as close to the purple line as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c114ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=\"auc = \" + str(round(auc,2)))\n",
    "x_ideal = (0, 0.001, 1)\n",
    "y_ideal = (0, 1, 1)\n",
    "plt.plot(x_ideal, y_ideal, color = \"violet\", label = \"Ideal\")\n",
    "plt.plot([0,1], [0,1], color = \"red\", linestyle = \"--\",\n",
    "          label = \"random\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af3db0",
   "metadata": {},
   "source": [
    "The performance of this is quantified by the Area Under the Curve (AUC) which shows how close the model is to the ideal performance. \n",
    "\n",
    "The “best” AUC score for the ideal model would be 1 where our random model has an AUC of 0.5. We can thus see our model performs relatively well although not perfect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd388a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af4ea959",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
