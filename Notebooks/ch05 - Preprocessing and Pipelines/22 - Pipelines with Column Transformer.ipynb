{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color: #C9C9C9'>Machine Learning with Python<img style=\"float: right; margin-top: 0;\" width=\"240\" src=\"../../Images/cf-logo.png\" /></h1> \n",
    "<p style='color: #C9C9C9'>&copy; Coding Fury 2022 - all rights reserved</p>\n",
    "\n",
    "<hr style='color: #C9C9C9' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "In the last example you may recall that we read in the James Bond dataset and trained a model.\n",
    "\n",
    "However, we ignored one of the columns - the actor who played bond was dropped. \n",
    "\n",
    "This is something we had to do because our pipeline wouldn't have been able to Scale and Centre this Column because it contains categorical information rather than numerical data. \n",
    "\n",
    "Ideally we'd have liked to process this column in a different way from the rest. \n",
    "\n",
    "In this tutorial we're going build a pipeline that processes both categorical and numerical data seperately using Column Transfomers. We'll scale and centre the numerical columns, and create dummies for the categorical columns. \n",
    "\n",
    "Remember that a Pipeline can have several steps that transform the data, but can only contain one ML model. Again we'll be using Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_df = pd.read_csv('../../Data/JamesBond.csv')\n",
    "bond_df = bond_df.drop('Movie', axis=1)\n",
    "bond_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bond_df.drop('Top_100', axis=1)\n",
    "y = bond_df['Top_100']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because I'm using a pipeline I don't have to convert X and Y to numpy arrays, I'm keeping them as dataframes.\n",
    "\n",
    "This also allows me to reference columns by name..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline([('one_hot', OneHotEncoder())])\n",
    "\n",
    "cat_features = ['Bond']\n",
    "num_features = ['Year', 'US_Gross', 'US_Adj', 'World_Gross', 'World_Adj', 'Budget',\n",
    "       'Budget_Adj', 'Film_Length', 'Avg_User_IMDB', 'Avg_User_Rtn_Tom',\n",
    "       'Conquests', 'Martinis', 'BJB', 'Kills_Bond', 'Kills_Others']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, num_features),\n",
    "        (\"cat\", categorical_transformer, cat_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "            ('model', LogisticRegression())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, we've improved the accuracy of our model by adding in the actor who played bond!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Further Reading\n",
    "\n",
    "For a similar example see:\n",
    "https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "\n",
    "This example has the added step of varying the Model in the pipeline at the end using a loop. \n",
    "\n",
    "# Even more Reading\n",
    "\n",
    "If you'd like to know how to debug a pipeline, there are several approaches you can take: \n",
    "https://www.google.com/search?q=debugging+a+scikit+learn+pipeline&oq=debugging+a+scikit+learn+pipeline&aqs=edge..69i57j0i546l4j69i64.11809j0j1&sourceid=chrome&ie=UTF-8\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
