{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color: #C9C9C9'>Machine Learning with Python<img style=\"float: right; margin-top: 0;\" width=\"240\" src=\"../../Images/cf-logo.png\" /></h1> \n",
    "<p style='color: #C9C9C9'>&copy; Coding Fury 2022 - all rights reserved</p>\n",
    "\n",
    "<hr style='color: #C9C9C9' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling by Normalising or Standardising your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling can be an issue for some models. \n",
    "\n",
    "By this I mean, if some features have much larger values than others, they'll have a heavier weighting than others. \n",
    "\n",
    "Thinking back to Hooke's Law, we gather data for how much a spring extends when we add weights to it.\n",
    "\n",
    "![Hooke's Law Experiment](../../Images/hookes-law.png)\n",
    "\n",
    "And we plot the results: \n",
    "![Hooke's Law Experiment](../../Images/hookes-law-graph-1.png)\n",
    "\n",
    "Next time, we repeat the experiment with an industrial grade spring that extends A LOT when each 1 Newtown weight added. \n",
    "\n",
    "![Hooke's Law Experiment](../../Images/hookes-law-graph-2.png)\n",
    "\n",
    "(It's probably obvious that I made up these numbers but please bear with me).\n",
    "\n",
    "Comparing the two charts: you should notice that in the second chart a small change in X produces a large change in y. Futhermore, this leads to a large coefficient in the $y=ax+b$ formula i.e. the value for a is large. \n",
    "\n",
    "Now consider what happens in multiple linear regression. If some of the features contain much larger numbers than others i.e. if some columns are orders of magnitude greater than others, this will lead to large cofficients on some features. \n",
    "\n",
    "For example, when linear regression is applied to a dataset, the formula might be: \n",
    "\n",
    " $$ y = 1.6x_1 + 192x_2 + b$$\n",
    "\n",
    " It should be obvious that a small change in $x_2$ will have a large impact on the target value, compared to a similar change in $x_1$. \n",
    "\n",
    " The solution is to Normalise the data by: \n",
    " * Centering \n",
    " * Scaling\n",
    "\n",
    " In order to center a column of data\n",
    "\n",
    "\n",
    " Models including Linear Regression, KNearestNeighbours, State Vector Machines and Neural Networks are all sensitive to this problem. \n",
    "\n",
    "# The Automobiles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df['peak_rpm'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ensure that we can see all columns when we display a dataframe\n",
    "pd.set_option('max_columns', None) \n",
    "\n",
    "# read the automobiles dataset into a dataframe\n",
    "auto_df = pd.read_csv('../../Data/automobiles.csv')\n",
    "\n",
    "# drop the symbolling and normalised losses columns\n",
    "auto_df = auto_df.drop(['symboling', 'normalised_losses'], axis=1)\n",
    "#Â drop all rows with na values\n",
    "auto_df = auto_df.dropna() \n",
    "auto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the automobiles dataset (above) we can see that the features do order by several orders of magnitude, and therefore, we should be applying strategies to Normalise or Standardise the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardising vs Normalising\n",
    "\n",
    "In this section we'll Standardise and then Normalise the 'stroke' and 'engine_size' columns so that you can compare and contrast the differences between them both. \n",
    "\n",
    "## Standardising\n",
    "\n",
    "Once a feature is standardised, most of its values fall in the range -1 to +1.\n",
    "\n",
    "This process invoves 2 steps. \n",
    "1. Centering\n",
    "2. Scaling\n",
    "\n",
    "Centering is achieved by calculating the mean for the column (or feature), and subtracting the mean from each value in the column. \n",
    "\n",
    "Scaling is achieved by dividing each number by the standard deviation. \n",
    "\n",
    "Standardising is usually the best choice when the data follows a gaussian distribution (bell curve). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(auto_df[['stroke', 'engine_size']])\n",
    "# Print out the Standardised Data as a Dataframe\n",
    "pd.DataFrame(X)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising \n",
    "\n",
    "Normalising is similar, however once a column (or feature) has been normalised, its values will fall in the region 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "X = scaler.fit_transform(auto_df[['stroke', 'engine_size']])\n",
    "# Print out the Standardised Data as a Dataframe\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of Regularised Regression\n",
    "\n",
    "At this point you may wish to recall Ridge Regression and Lasso Regression both of which can be used to shrink large coefficients.  In the case of Lasso Regression some coefficients may even shrink to zero (which is feature selection). \n",
    "\n",
    "For this reason we don't employee Normalisation or Standardisation with Ridge or Lasso Regression - scaling features is already baked in.\n",
    "\n",
    "One of the main advantages of being able to use Standardisation or Normalisation yourself is that you can apply it to any model.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
