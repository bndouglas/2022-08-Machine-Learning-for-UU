{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color: #C9C9C9'>Machine Learning with Python<img style=\"float: right; margin-top: 0;\" width=\"240\" src=\"../../Images/cf-logo.png\" /></h1> \n",
    "<p style='color: #C9C9C9'>&copy; Coding Fury 2022 - all rights reserved</p>\n",
    "\n",
    "<hr style='color: #C9C9C9' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Automobiles\n",
    "\n",
    "* Load the Automobiles Dataset\n",
    "* Drop any rows for which we don't have a price\n",
    "* Drop the 'symbolling' and 'normalised_losses' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# ensure that we can see all columns when we display a dataframe\n",
    "pd.set_option('max_columns', None) \n",
    "\n",
    "# read the automobiles dataset into a dataframe\n",
    "auto_df = pd.read_csv('../../Data/automobiles.csv')\n",
    "\n",
    "# drop rows that are missing a value for price\n",
    "auto_df = auto_df.dropna(subset=['price']) \n",
    "\n",
    "# drop the symbolling and normalised losses columns\n",
    "auto_df = auto_df.drop(['symboling', 'normalised_losses'], axis=1)\n",
    "auto_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mode = SimpleImputer(strategy='most_frequent', missing_values=np.NaN)   # \"most_frequent\" is same as the \"mode\"\n",
    "auto_df['num_of_doors'] = imp_mode.fit_transform(auto_df[['num_of_doors']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(strategy='mean', missing_values=np.NaN)  \n",
    "auto_df[['bore', 'stroke']] = imp_mean.fit_transform(auto_df[['bore','stroke']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_median = SimpleImputer(strategy='median', missing_values=np.NaN)  \n",
    "auto_df[['horsepower','peak_rpm']] = imp_mean.fit_transform(auto_df[['horsepower','peak_rpm']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardise all the number columns\n",
    "\n",
    "Note that I have to name the columns because I don't want to Standardise the columns I'm making dummies with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cols = ['wheel_base','length','width','height','curb_weight', 'engine_size', 'bore','stroke','compression_ratio','horsepower','peak_rpm','city_mpg','highway_mpg']\n",
    "features = auto_df[cols]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('ct_scaler', StandardScaler(), cols)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "auto_df[cols] = ct.fit_transform(features)\n",
    "auto_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dummy Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_dummies_df = pd.get_dummies(auto_df, drop_first=True)\n",
    "auto_dummies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Score the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = auto_dummies_df.drop('price', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = auto_dummies_df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discuss\n",
    "\n",
    "Note that we fudged this example slightly. We should really have split the data into test and training, then we should have scaled each one separately. \n",
    "\n",
    "We've skipped this step in the interest of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
