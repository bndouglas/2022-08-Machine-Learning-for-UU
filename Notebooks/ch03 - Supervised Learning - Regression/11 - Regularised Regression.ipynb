{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23dcca9b-3523-453e-9cc7-26bfcaa0f018",
   "metadata": {},
   "source": [
    "<h1 style='color: #C9C9C9'>Machine Learning with Python<img style=\"float: right; margin-top: 0;\" width=\"240\" src=\"../../Images/cf-logo.png\" /></h1> \n",
    "<p style='color: #C9C9C9'>&copy; Coding Fury 2022 - all rights reserved</p>\n",
    "\n",
    "<hr style='color: #C9C9C9' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8a3fd-0c32-4e83-b2b0-f0962f59b9da",
   "metadata": {},
   "source": [
    "# Regularised Regression\n",
    "\n",
    "\n",
    "Thinking back to the equation for linear regression, large coefficients can lead to overfitting. \n",
    "\n",
    "$y = a_1x + a_2x + b$\n",
    "\n",
    "Regularisation is used in combination with the loss function to penalise models with large coefficients e.g. $a_1$ or $a_2$ in the equation above. \n",
    "\n",
    "In other words, we can use Regularised Regression to make our model less sensitive to changes in certain input features. \n",
    "\n",
    "We're going to look at 2 types of Regularised Regression: \n",
    "1. Ridge Regression\n",
    "2. Lasso Regression\n",
    "\n",
    "\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "In an earlier lesson we covered Linear Regression, and calculated the Loss Function using Ordinary Least Squares. \n",
    "\n",
    "Ridge regression (aka Tikhonov regularisation) modifies the loss function by adding a penalty to the square of the coefficients. \n",
    "\n",
    "$$ loss function = (OLS)\\ loss\\ function + \\alpha \\sum_{i=1}^{n} {a_i}^2 $$\n",
    "\n",
    "> OLS = Ordinary Least Squares\n",
    "\n",
    "We can then tune the loss function by adjusting $\\alpha$ in order to find the right balance between fitting our data well, and overfitting. \n",
    "\n",
    "![](../../Images/overfitting.png)\n",
    "\n",
    "$\\alpha$ is a hyperparameter. Its value is used to control the learning process. In a similar way to k in the kNearestNeighbours algorithm.\n",
    "\n",
    "When $\\alpha=0$ we are performing normal OLS, but this might lead to overfitting.\n",
    "\n",
    "When $\\alpha$ is high, large coefficients are penalised, but this might lead to underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bodyfat_df = pd.read_csv('../../Data/bodyfat.txt', delimiter='\\t')\n",
    "bodyfat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208cb851",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bodyfat_df.drop(\"Bodyfat\", axis=1).values\n",
    "y = bodyfat_df[\"Bodyfat\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef34cc-6167-476a-bd9a-b9fab2afc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "scores = []\n",
    "for alpha in [0, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_pred = ridge.predict(X_test) \n",
    "    scores.append(ridge.score(X_test, y_test))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c9892",
   "metadata": {},
   "source": [
    "In this case the performance $R^2$ gets worse as $\\alpha$ increases. Remember that if $\\alpha = 0$ we're just using OLS without any Ridge Regression Penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee1125",
   "metadata": {},
   "source": [
    "## In what scenarios is Ridge Regression Useful? \n",
    "\n",
    "\n",
    "### Scenario 1\n",
    "\n",
    "You only have a small training data set available, and there is a high variance in the Loss Function and you're getting very different $R^2$ values for different test sets of data. \n",
    "\n",
    "With Ridge Regression you can introduce Bias into the model, which means that the model fits less well to the training set, but works better overall, across many test datasets.\n",
    "\n",
    "If you want to know more about bias and variance in Machine Learning, watch this [video by StatQuest](https://www.youtube.com/watch?v=EuBBz3bI-aA). \n",
    "\n",
    "### Scenario 2\n",
    "\n",
    "If some features have high coefficients this may lead to overfitting. Ridge Regression penalises these features. \n",
    "\n",
    "\n",
    "### Scenario 3\n",
    "\n",
    "We have a large number of features, but a small training dataset. \n",
    "\n",
    "E.g. we want to see what effect 10,000 different genes have on obesity, but we only have 5000 records to train the model with.\n",
    "\n",
    "#### Explaining the problem with Scenario 3\n",
    "\n",
    "If we want to perform linear regression on a dataset with 2 dimensional data (1 feature, 1 target variable) we need at least 2 points (2 observations) to plot a line. \n",
    "\n",
    "For 2 features and a target variable we need at least 3 data points (3 observations), because we're now plotting a plane.\n",
    "\n",
    "For 3 features we're plotting a 3d shape so we need at least 4 observations. \n",
    "\n",
    "For 10,000 features, we need at least 10,001 observations. A Linear Regression model would fail if you try any fewer. \n",
    "\n",
    "This problem is more common than you think. Ridge Regression and Cross Validation will allow you to successfully train a model with fewer observations than features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a99c02",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0cbd7",
   "metadata": {},
   "source": [
    "The main difference between Ridge and Lasso Regression is how they deal with negative values of the linear regression coefficient(s) a. \n",
    "\n",
    "* Ridge regression squares values of a.\n",
    "* Lasso regression takes the modulus of a.\n",
    "\n",
    "Loss function = OLS loss function + $ \\alpha \\sum_{i=1}^{n} |{a_i}| $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8695df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bodyfat_df = pd.read_csv('../../Data/bodyfat.txt', delimiter='\\t')\n",
    "bodyfat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07547f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bodyfat_df.drop(\"Bodyfat\", axis=1).values\n",
    "y = bodyfat_df[\"Bodyfat\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fef122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "scores = []\n",
    "alphas = np.arange(0.5,20, 0.5)\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    y_pred = lasso.predict(X_test)\n",
    "    scores.append(lasso.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141bb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(alphas, scores)\n",
    "plt.xlabel(\"aplha\")\n",
    "plt.ylabel(\"score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b98f3",
   "metadata": {},
   "source": [
    "# Challenges: \n",
    "\n",
    "Once again we find that by increasing alpha for this dataset we're actually getting worse results, but that's not always the case.\n",
    "\n",
    "Go back to where you trained your model, and change the random state from 42, to 12.\n",
    "\n",
    "Do you get a different Outcome? \n",
    "\n",
    "**What next?**\n",
    "\n",
    "A good approach would be to try k-folds cross validation to find the best value of alpha that works across multiple folds. \n",
    "\n",
    "However, you should be aware that there are only 20 observations in this dataset, so perhaps that's something I can leave you to circle back and try in the future with a larger dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd9022",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adc6876e",
   "metadata": {},
   "source": [
    "# Lasso Regression for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.1)\n",
    "lasso_coef = lasso.fit(X, y).coef_ \n",
    "lasso_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "column_names = bodyfat_df.drop(\"Bodyfat\", axis=1).columns\n",
    "plt.bar(column_names, lasso_coef)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963f99d",
   "metadata": {},
   "source": [
    "This chart shows that Triceps is the most import feature. \n",
    "\n",
    "It also shows that Lasso Regression (like Ridge Regression), doesn't shrink all features equally. \n",
    "\n",
    "The main difference between Ridge and Lasso Regression is that Lasso shrink some features to zero; whereas Ridge Regression can only shrink them close to zero. \n",
    "\n",
    "Essentially, this means that Lasso Regression is better at getting rid of useless features from your dataset. \n",
    "\n",
    "By contrast Ridge Regression tends to perform slightly better than Lasso if most features are useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b3df8",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "How does Lasso feature selection compare to analyzing using a Seaborn PairPlot as shown below? \n",
    "\n",
    "While Lasso Regression successfully identified that Triceps have a strong correlation to Bodyfat, it did less well with the Thigh and Midarm. Would you like to comment? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(bodyfat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d7aa3",
   "metadata": {},
   "source": [
    "The chart above shows that Tricep thickness is the feature that has the greatest influence on the bodyfat. \n",
    "\n",
    "The medical profession often uses tricep skin thickness as a metric to measure obseity, so it seems likely they already know this. It's good that our data concur!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98324eb4",
   "metadata": {},
   "source": [
    "# Congratulations\n",
    "\n",
    "## You know know how to Lasso!\n",
    "\n",
    "![Lasso](../../Images/Lasso.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854503c4",
   "metadata": {},
   "source": [
    "# Further Learning\n",
    "\n",
    "StatQuest : Bias and Variance : https://www.youtube.com/watch?v=EuBBz3bI-aA \n",
    "\n",
    "StatQuest : Ridge Regression : https://www.youtube.com/watch?v=Q81RR3yKn30&t=288s \n",
    "\n",
    "StatQuest : Lasso Regression : https://www.youtube.com/watch?v=NGf0voTMlcs\n",
    "\n",
    "Note: StatQuest denotes the penalty hyperparameter with $\\lambda$ where we've used $\\alpha$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f8e31-e06d-4aea-a3f7-fe4e19ffa557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
